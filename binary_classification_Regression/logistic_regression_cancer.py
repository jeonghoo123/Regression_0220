# -*- coding: utf-8 -*-
"""Logistic_Regression_cancer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OLi1Aq8pvSQBHdH8osYl1r2PhaQGkaXa
"""

#종양데이터 분석하
# 데이터셋은 sklearn에서 인출
import sklearn
data = sklearn.datasets.load_breast_cancer()
print(type(data))

print(data.keys())

print(data["data"])
print(data["target"])
print(data["target_names"])
print(data["DESCR"])
print(data["feature_names"])
#[malignant(악성종양), benign(양성종양)]
#[radius(반지름),texture(표면조직),perimeter(둘레),area(범위),smoothness(부드러움),
# compactness(컴팩트),concavity(오목함),concave points(오목점), symmetry(대칭성),
#  fractal dimension(프랙탈차원)
#]

x_data = data["data"]#[mean, error, worst]
y_data = data["target"]
print(x_data.shape)
print(y_data.shape)

import numpy as np
#최적화 필요 여부 확인

for c in range(30):
  print("최대값:",np.max(x_data[:,c]))
for c in range(30):
  print("편차:",np.std(x_data[:,c]))
#값의 편차들이 크므로 최적화 필요

#훈련데이터(80%), 검증데이터(10%), 테스트 데이터(10%)
x_train,x_valid,y_train,y_valid = \
    sklearn.model_selection.train_test_split(x_data,y_data,\
    test_size=0.1,random_state=111,stratify=y_data)

#테스트 데이터 분리
x_train,x_test,y_train,y_test = \
    sklearn.model_selection.train_test_split(x_train,y_train,\
    test_size=0.1,random_state=111,stratify=y_train)
print(x_train.shape)
print(x_valid.shape)
print(x_test.shape)
print(y_train.shape)
print(y_valid.shape)
print(y_test.shape)

from sklearn import preprocessing
scaler = preprocessing.StandardScaler()
x_train=scaler.fit_transform(x_train)#표준편차와 평균을 구하여 정규분포화

print(np.std(x_train[:,3]))

x_test = scaler.transform(x_test)# 훈련데이터의 편차와 평균으로 정규분포
x_valid = scaler.transform(x_valid)# 훈련데이터의 편차와 평균으로 정규분포
#전처리 끝.

print(np.std(x_train[:,3]))
print(np.std(x_test[:,3]))
print(np.std(x_valid[:,3]))

#최종 데이터 확인
print(x_train.shape)
print(x_valid.shape)
print(x_test.shape)
print(y_train.shape)
print(y_valid.shape)
print(y_test.shape)
print(np.max(x_train))
print(np.max(x_valid))
print(np.max(x_test))

#모델 구성
import tensorflow as tf
from tensorflow.keras import Sequential, Input
from tensorflow.keras.layers import Dense
model = Sequential()
model.add(Dense(1,activation="sigmoid"))

model.compile(loss="binary_crossentropy",optimizer="SGD",metrics=["accuracy"])
fit_his = model.fit(x_train,y_train,\
                    validation_data=(x_valid,y_valid),epochs=200,batch_size=30)

print(fit_his.history.keys())

import matplotlib.pyplot as plt
plt.subplot(1,2,1)
plt.plot(fit_his.history["loss"],label="train")
plt.plot(fit_his.history["val_loss"],label="valid")
plt.legend()
plt.title("LOSSES")
plt.subplot(1,2,2)
plt.plot(fit_his.history["accuracy"],label="train")
plt.plot(fit_his.history["val_accuracy"],label="valid")
plt.legend()
plt.title("ACCURACY")

#모델의 평가
result=model.evaluate(x_test,y_test)

#평가결과
print(f"모델의 손실율:{result[0]*100:.2f}%")
print(f"모델의 정확률:{result[1]*100:.2f}%")

#예측값 인출 평가
y_pred = model.predict(x_test)
print(y_pred.shape)
print(y_test.shape)# 0 1
y_test = y_test.reshape((len(y_test),-1))
print(y_test.shape)
y_pred = (y_pred[:]>=0.5) # False True
y_pred = y_pred.astype(np.int8)
print(y_pred.shape)
print(y_pred[0])
print(y_test[0])
right=0
wrong=0
for i in range(len(y_test)):
  if y_pred[i][0]==y_test[i][0]:
    right+=1
  else : wrong+=1
percent = (1-wrong/right)*100
print(f"{percent:.2f}%")

# 혼동행렬 confusion metrix
cm = sklearn.metrics.confusion_matrix(y_test,y_pred)
print(cm)

# heatmap
import seaborn as sns
ax = sns.heatmap(cm, annot=True)
ax.set(xlabel="pred", ylabel="true")
plt.show()

# f1 score
fscore = sklearn.metrics.f1_score(y_test,y_pred)
print(fscore)

report = sklearn.metrics.classification_report(y_test,y_pred)
print(report)

